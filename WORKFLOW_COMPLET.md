# Workflow Complet - Analytics Automation üöÄ

**Date:** 2026-02-15
**Version:** 1.0

---

## üéØ Objectif

Pipeline automatis√© de A‚ÜíZ pour analytics et reporting:
1. Connexion BigQuery ‚úÖ
2. R√©conciliation API vs BigQuery ‚úÖ
3. D√©tection nouvelles tables ‚úÖ
4. V√©rification freshness ‚úÖ
5. **Encoding PII coh√©rent** (m√™me email = m√™me hash partout) ‚úÖ
6. Unification tables (d√©doublonnage) ‚úÖ
7. D√©tection anomalies (NULL, 0, data manquante) ‚úÖ
8. G√©n√©ration rapport ex√©cutif (26 sections) ‚úÖ

---

## ‚ö° Quick Start

### Option 1: Tout Ex√©cuter (Recommand√©)

```bash
cd ~/Documents/Projects/Dev_Ops
python3 scripts/master_workflow.py
```

**Dur√©e:** ~5-10 minutes
**R√©sultat:** Pipeline complet ex√©cut√© + rapport PowerPoint

---

### Option 2: Ex√©cution Partielle

```bash
# Skip r√©conciliation (si d√©j√† fait)
python3 scripts/master_workflow.py --skip-reconciliation

# Skip PII encoding (si d√©j√† fait)
python3 scripts/master_workflow.py --skip-pii

# Skip rapport (focus sur data seulement)
python3 scripts/master_workflow.py --skip-report

# Combiner plusieurs skips
python3 scripts/master_workflow.py --skip-reconciliation --skip-report
```

---

## üìã D√©tail des 8 √âtapes

### √âtape 1: Test Connexion BigQuery üîå

**Objectif:** V√©rifier que tu peux acc√©der √† BigQuery

**Commande:**
```bash
bq ls --project_id=hulken ads_data
```

**R√©sultat attendu:** Liste des tables dans `ads_data`

**Si erreur:**
```bash
gcloud auth application-default login
```

---

### √âtape 2: R√©conciliation API vs BigQuery üîÑ

**Objectif:** Comparer les donn√©es dans les APIs (Shopify, Facebook, etc.) avec BigQuery

**Script:** `data_validation/live_reconciliation.py`

**V√©rifications:**
- Shopify: Orders dans API = Orders dans BigQuery?
- Facebook: Spend dans Ads Manager = Spend dans BigQuery?
- TikTok: M√©triques coh√©rentes?
- Google Ads: Conversions matchent?

**R√©sultat:** Rapport de discordances (si des donn√©es manquent)

---

### √âtape 3: D√©tection Nouvelles Tables üîç

**Objectif:** Identifier si Airbyte a ajout√© de nouvelles tables

**Script:** `data_validation/table_monitoring.py`

**V√©rifications:**
- Compare baseline (derni√®res tables connues) vs tables actuelles
- D√©tecte tables vides (0 lignes)
- D√©tecte syncs stale (>48h sans update)

**Exemple output:**
```
NEW tables detected:
  - shopify_live_metafields (0 rows) ‚Üê Nouveau!
  - facebook_ads_insights_dma (1,234 rows) ‚Üê Nouveau!

STALE tables (>48h):
  - tiktok_ads_reports_daily (last sync: 3 days ago)
```

---

### √âtape 4: V√©rification Freshness des Donn√©es ‚è∞

**Objectif:** S'assurer que les donn√©es sont √† jour

**Requ√™te SQL:**
```sql
SELECT
  table_id,
  TIMESTAMP_MILLIS(last_modified_time) AS last_sync,
  TIMESTAMP_DIFF(CURRENT_TIMESTAMP(), TIMESTAMP_MILLIS(last_modified_time), HOUR) AS hours_since_sync
FROM `hulken.ads_data.__TABLES__`
WHERE table_id IN (
  'shopify_live_orders',
  'facebook_ads_insights',
  'tiktok_ads_reports_daily'
)
ORDER BY hours_since_sync DESC;
```

**Seuil d'alerte:** >48 heures

**Action si stale:**
1. Aller dans Airbyte
2. Forcer un sync manuel
3. V√©rifier les logs d'erreur

---

### √âtape 5: Encoding PII Coh√©rent üîê

**Objectif:** Garantir que le M√äME email a le M√äME hash dans toutes les tables

**Probl√®me √† r√©soudre:**
- Email `john@example.com` dans Shopify ‚Üí Hash `abc123`
- MAIS email `john@example.com` dans Facebook ‚Üí Hash `xyz789` ‚ùå

**Solution:**

1. **Cr√©er une table de r√©f√©rence:**
   ```sql
   CREATE OR REPLACE TABLE `hulken.ads_data.pii_hash_reference` AS

   WITH all_emails AS (
     SELECT DISTINCT email_hash FROM shopify_live_customers
     UNION DISTINCT
     SELECT DISTINCT email_hash FROM shopify_live_orders
     UNION DISTINCT
     SELECT DISTINCT customer_email_hash FROM facebook_customers
   )

   SELECT
     email_hash AS email_hash_original,
     TO_HEX(SHA256(email_hash)) AS email_hash_consistent
   FROM all_emails;
   ```

2. **Utiliser cette table partout:**
   ```sql
   -- Dans shopify_unified
   SELECT
     o.*,
     ref.email_hash_consistent  -- ‚Üê Hash coh√©rent
   FROM shopify_live_orders o
   LEFT JOIN pii_hash_reference ref
     ON o.email_hash = ref.email_hash_original;
   ```

**R√©sultat:** M√™me email = m√™me hash dans TOUTES les tables

---

### √âtape 6: Unification des Tables (Sans Doublons) üîó

**Objectif:** Cr√©er tables unifi√©es avec d√©doublonnage

**Script:** `sql/create_unified_tables.sql`

**Tables cr√©√©es:**
1. **shopify_unified** - Merge de:
   - shopify_live_orders_clean (base)
   - shopify_live_customers_clean (via email_hash)
   - shopify_line_items (via order_id)
   - shopify_live_transactions (via order_id)
   - shopify_utm (via order_id)
   - shopify_live_order_refunds (via order_id)

2. **facebook_unified** - Facebook Ads m√©triques

3. **tiktok_unified** - TikTok Ads m√©triques

4. **google_ads_unified** - Google Ads m√©triques

5. **marketing_unified** - MASTER TABLE (tout combin√©)

**D√©doublonnage:**
```sql
-- V√©rification automatique des doublons
SELECT
  'shopify_unified' AS table_name,
  COUNT(*) AS total_rows,
  COUNT(DISTINCT order_id) AS unique_orders,
  COUNT(*) - COUNT(DISTINCT order_id) AS duplicates
FROM shopify_unified;
```

**Si duplicates > 0:** Alerte g√©n√©r√©e!

---

### √âtape 7: D√©tection d'Anomalies üö®

**Objectif:** Trouver les donn√©es NULL ou 0 inappropri√©es

**Types d'anomalies d√©tect√©es:**

#### A. NULL inappropri√©s
```sql
-- Orders sans order_value (illogique!)
SELECT COUNT(*)
FROM shopify_unified
WHERE order_value IS NULL;

-- Orders sans customer_id (possible mais rare)
SELECT COUNT(*)
FROM shopify_unified
WHERE customer_id IS NULL;
```

#### B. Z√©ros suspects
```sql
-- Revenue = 0 mais ad_spend > 0 (suspect!)
SELECT date, channel, ad_spend, revenue
FROM marketing_unified
WHERE revenue = 0 AND ad_spend > 0;
```

#### C. Donn√©es manquantes vs historique
```sql
-- Compare vs moyenne des 30 derniers jours
WITH avg_last_30d AS (
  SELECT AVG(revenue) AS avg_revenue
  FROM marketing_unified
  WHERE date >= DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)
)

SELECT date, revenue
FROM marketing_unified, avg_last_30d
WHERE revenue < (avg_revenue * 0.5)  -- 50% sous la moyenne
  AND date >= DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY);
```

**R√©sultat:** Rapport d'anomalies sauvegard√© dans `logs/anomalies_YYYYMMDD.txt`

---

### √âtape 8: G√©n√©ration Rapport Ex√©cutif üìä

**Objectif:** Cr√©er PowerPoint avec les 26 sections

**Script:** `scripts/generate_powerpoint.py`

**Fichier g√©n√©r√©:** `reports/Marketing_Performance_Report.pptx`

**Sections incluses:**

#### Section 1: Total Business Performance (2 slides)
- Executive Summary (KPIs avec YoY)
- Marketing Efficiency

#### Section 2: DTC Performance (11 slides)
- Sitewide Overview
- Traffic & Sales Trends
- Conversion & Revenue Efficiency
- Marketing Cost Efficiency
- New vs Returning Customers
- Search Demand Trends ‚ö†Ô∏è (Google Trends requis)
- Funnel Measurement
- Merchandising Performance
- Content & UX ‚ö†Ô∏è (GA4 requis)
- Geographic Insights
- International (Canada)
- Demographics & Devices ‚ö†Ô∏è (GA4 requis)

#### Section 3: Amazon Performance (3 slides)
‚ö†Ô∏è Requiert Amazon Ads connect√©

#### Section 4: Paid Marketing (6 slides)
- Paid Channel Mix
- PPC Performance Table
- Creative Performance ‚ö†Ô∏è (Motion requis)
- Landing Page Performance ‚ö†Ô∏è (GA4 requis)
- Reach & Saturation ‚ö†Ô∏è (Facebook Ads Manager)
- Search Query Performance ‚ö†Ô∏è (Google Ads query data)

#### Section 5: Customer Voice (2 slides)
‚ö†Ô∏è Requiert Fairing survey data

#### Appendix (1 slide)
- Total Business PPC Performance Index

**Total:** 23 slides g√©n√©r√©es, 20/26 sections avec donn√©es actuelles (77%)

---

## üîÑ Workflow Automatis√© Quotidien

### Setup Cron Job (Ex√©cution Automatique)

```bash
# Ouvrir crontab
crontab -e

# Ajouter cette ligne (ex√©cute √† 6h du matin tous les jours)
0 6 * * * cd /Users/raphael_sebbah/Documents/Projects/Dev_Ops && python3 scripts/master_workflow.py >> logs/workflow_cron.log 2>&1

# Sauvegarder et quitter (:wq)
```

**R√©sultat:** Chaque matin √† 6h:
1. Toutes les v√©rifications sont faites
2. Tables sont unifi√©es
3. Anomalies sont d√©tect√©es
4. Rapport PowerPoint est g√©n√©r√©

---

## üìä Dashboards Looker Studio

**En compl√©ment du PowerPoint, cr√©er dashboards Looker:**

### Dashboard 1: Executive Summary
- Source: `executive_summary_monthly`
- KPIs: Revenue, Spend, ROAS, Orders
- Graphique: Trend mensuel
- Filtres: Date range, Channel

### Dashboard 2: Shopify Performance
- Source: `shopify_daily_metrics`
- KPIs: Orders, AOV, Returning %
- Graphiques: Traffic trend, Top products
- Filtres: Date, Product category

### Dashboard 3: Channel Performance
- Source: `channel_mix`
- Pie chart: Spend distribution
- Table: Channel performance (ROAS, CPA, etc.)
- Filtres: Date, Channel status

### Dashboard 4: Anomalies
- Source: Custom query avec anomaly detection logic
- Alerts: Tables NULL/0 suspects
- Trend: Anomalies over time

**Quick start Looker:** [docs/LOOKER_10MIN_QUICKSTART.md](docs/LOOKER_10MIN_QUICKSTART.md)

---

## üö® Troubleshooting

### Erreur: "BigQuery connection failed"
```bash
gcloud auth application-default login
gcloud config set project hulken
```

### Erreur: "Table not found"
V√©rifier que les tables existent:
```bash
bq ls --project_id=hulken ads_data
```

### Erreur: "Permission denied"
V√©rifier les permissions BigQuery:
```bash
gcloud projects get-iam-policy hulken --flatten="bindings[].members" --format="table(bindings.role)" --filter="bindings.members:$(gcloud config get-value account)"
```

R√¥le requis: `roles/bigquery.dataEditor` ou `roles/bigquery.admin`

### Anomalies d√©tect√©es mais normales
√âditer le script `master_workflow.py`:
```python
# Ligne ~XXX - Ajuster le seuil
WHERE null_count > 0 OR zero_count > (total_rows * 0.1)  # 10% ‚Üí Changer √† 20%
```

---

## üìà KPIs de Monitoring du Workflow

**√Ä surveiller quotidiennement:**

| M√©trique | Seuil OK | Seuil Warning | Seuil Critical |
|----------|----------|---------------|----------------|
| Freshness (hours) | < 24h | 24-48h | > 48h |
| Duplicates | 0 | 1-10 | > 10 |
| NULL % | < 5% | 5-15% | > 15% |
| Anomalies count | 0 | 1-5 | > 5 |
| Workflow duration | < 5 min | 5-10 min | > 10 min |

---

## üéØ Next Steps

### Court terme (Cette semaine)
1. ‚úÖ Ex√©cuter le workflow une premi√®re fois
2. ‚ö†Ô∏è Fixer les anomalies d√©tect√©es
3. üìä Cr√©er le dashboard Looker Studio
4. üîÑ Setup cron job pour automatisation

### Moyen terme (Ce mois)
1. Ajouter Amazon Ads (guide: [docs/AMAZON_ADS_AIRBYTE_SETUP.md](docs/AMAZON_ADS_AIRBYTE_SETUP.md))
2. Connecter GA4 pour sessions/devices
3. Connecter Fairing pour surveys
4. Connecter Motion pour creative performance

### Long terme (Ce trimestre)
1. Machine Learning pour d√©tection d'anomalies avanc√©e
2. Pr√©dictions ROAS par canal
3. Budget allocation optimization
4. Customer LTV prediction

---

## üìö Fichiers Li√©s

| Fichier | Description |
|---------|-------------|
| **[WORKFLOW_COMPLET.md](WORKFLOW_COMPLET.md)** | Ce fichier - Workflow complet |
| **[scripts/master_workflow.py](scripts/master_workflow.py)** | Script orchestrateur principal |
| **[sql/create_unified_tables.sql](sql/create_unified_tables.sql)** | Unification des tables |
| **[data_validation/live_reconciliation.py](data_validation/live_reconciliation.py)** | R√©conciliation API vs BQ |
| **[data_validation/table_monitoring.py](data_validation/table_monitoring.py)** | Monitoring tables |
| **[scripts/generate_powerpoint.py](scripts/generate_powerpoint.py)** | G√©n√©ration PowerPoint |
| **[docs/LOOKER_10MIN_QUICKSTART.md](docs/LOOKER_10MIN_QUICKSTART.md)** | Quick start Looker |

---

## ‚úÖ Checklist Premi√®re Ex√©cution

- [ ] BigQuery access configur√© (`gcloud auth`)
- [ ] Tables unifi√©es cr√©√©es (shopify_unified, marketing_unified, etc.)
- [ ] Vues de reporting cr√©√©es (shopify_daily_metrics, channel_mix, etc.)
- [ ] Workflow ex√©cut√© avec succ√®s (`python3 master_workflow.py`)
- [ ] Anomalies v√©rifi√©es et corrig√©es
- [ ] PowerPoint g√©n√©r√© et v√©rifi√©
- [ ] Dashboard Looker cr√©√© (optionnel mais recommand√©)
- [ ] Cron job configur√© pour automation (optionnel)

---

## üéâ R√©sultat Final

**Apr√®s premi√®re ex√©cution compl√®te, tu auras:**

1. ‚úÖ **Tables BigQuery** propres, unifi√©es, sans doublons
2. ‚úÖ **PII encoding coh√©rent** (m√™me hash partout)
3. ‚úÖ **D√©tection automatique** des nouvelles tables et anomalies
4. ‚úÖ **PowerPoint professionnel** (23 slides, 77% des sections)
5. ‚úÖ **Workflow automatisable** (cron job quotidien)
6. ‚úÖ **Dashboard Looker** (optionnel, recommand√©)
7. ‚úÖ **Logs d'audit** pour tra√ßabilit√©

**Temps total:** ~10 minutes pour premier run, puis 5 minutes/jour en automatique

üöÄ **Ready to scale!**

